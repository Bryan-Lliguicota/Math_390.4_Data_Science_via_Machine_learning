knitr::opts_chunk$set(error = TRUE) #this allows errors to be printed into the PDF
pacman::p_load(testthat)
Xy_simple = data.frame(
response = factor(c(0, 0, 0, 1, 1, 1)), #nominal
first_feature = c(1, 1, 2, 3, 3, 4),    #continuous
second_feature = c(1, 2, 1, 3, 4, 3)    #continuous
)
X_simple_feature_matrix = as.matrix(Xy_simple[, 2 : 3])
y_binary = as.numeric(Xy_simple$response == 1)
perceptron_learning_algorithm = function(Xinput, y_binary, MAX_ITER = 1000, w = NULL){
if(is.null(w)){
w<- runif(ncol(Xinput))
}
for( n in 1:MAX_ITER){
for(i in 1:nrow(Xinput)){
x <- Xinput[i,]
y_i<-ifelse((w %*% x) >0 , 1 ,0)
w<- w + as.numeric(y_binary[i]-y_i)*x
}
}
w
}
w_vec_simple_per = perceptron_learning_algorithm(
cbind(1, Xy_simple$first_feature, Xy_simple$second_feature),
as.numeric(Xy_simple$response == 1))
w_vec_simple_per
pacman::p_load(ggplot2)
simple_viz_obj = ggplot(Xy_simple, aes(x = first_feature, y = second_feature, color = response)) +
geom_point(size = 5)
simple_perceptron_line = geom_abline(
intercept = -w_vec_simple_per[1] / w_vec_simple_per[3],
slope = -w_vec_simple_per[2] / w_vec_simple_per[3],
color = "orange")
simple_viz_obj + simple_perceptron_line
pacman::p_load(e1071)
?svm
svm_model = svm(X_simple_feature_matrix,y_binary,kernel="linear",scale=FALSE)
w_vec_simple_svm = c(
svm_model$rho, #the b term
-t(svm_model$coefs) %*% X_simple_feature_matrix[svm_model$index, ] # the other terms
)
simple_svm_line = geom_abline(
intercept = -w_vec_simple_svm[1] / w_vec_simple_svm[3],
slope = -w_vec_simple_svm[2] / w_vec_simple_svm[3],
color = "purple")
simple_viz_obj + simple_perceptron_line + simple_svm_line
rm(list=ls())
dist_form=function(x,y){
s<-sum((x-y)**2)
sqrt(s)
}
nn_algorithm_predict = function(Xinput, y_binary, Xtest){
n <-nrow(Xinput)
output_vec<-rep(Inf,n)
correct_y_index<-0
for(k in 1:n){
for(i in 1:n){
#compare the distance b/w Xtest and Xinput[1,]
tmp<- dist_form(Xinput[i,],Xtest[k,])
if(tmp < output_vec[k]){
output_vec[k]=tmp
correct_y_index<-i
}
}
output_vec[k]=y_binary[correct_y_index]
}
output_vec
}
Xy = na.omit(MASS::biopsy) #The "breast cancer" data with all observations with missing values dropped
X = Xy[, 2 : 10] #V1, V2, ..., V9
y_binary = as.numeric(Xy$class == "malignant")
X<-X[c(-61:-683),]
y_binary = y_binary[1:60]
y_hat = nn_algorithm_predict(X,y_binary,X)
y_hat = nn_algorithm_predict(X,y_binary,X)
all.equal(y_hat, y_binary)
rm(list=ls())
n = 20
x = runif(n)
beta_0 = 3
beta_1 = -2
y = beta_0 + beta_1 * x + rnorm(n, mean = 0, sd = 0.33)
something_bar= function(x){
n= length(x)
sum(x)/n
}
sd_variable=function(x){
n= length(x)
pt1_num<-sum(x**2)
pt2_num<-n*(something_bar(x))**2
num<- pt1_num - pt2_num
den<- n-1
sqrt(num/den)
}
my_cov_like_func=function(x,y){
#assumption: both  x and y have the same number of elements
n<-length(x)
pt1_num<-sum(x*y)
pt2_num<-n*something_bar(x)*something_bar(y)
num<-pt1_num-pt2_num
den<-(n-1)
num/den
}
r<- my_cov_like_func(x,y)/(sd_variable(x)*sd_variable(y))
b_1<-r*(sd_variable(y)/sd_variable(x))
b_0<- something_bar(y)-(b_1 * something_bar(x))
simple_linear_model_formula = y ~ x
lm_mod = lm(simple_linear_model_formula)
b_vec = coef(lm_mod)
expect_equal(b_0, as.numeric(b_vec[1]), tol = 1e-4) #thanks to Rachel for spotting this bug - the b_vec must be casted to numeric
expect_equal(b_1, as.numeric(b_vec[2]), tol = 1e-4)
rm(list=ls())
pacman::p_load(HistData)
Galton_data<-Galton
data(Galton)
summary(Galton_data)
summary(Galton_data)$parent$"1st Qu."
summary(Galton_data)$parent
summary(Galton_data).parent
summary(Galton_data)[parent]
summary(Galton_data)['parent']
summary(Galton_data)
summary(Galton_data)
dim(summary(Galton_data))
summary(Galton_data)
summary(Galton_data)[1,2]
summary(Galton_data)['parent']
summary(Galton_data)['parent']
summary(Galton_data)['parent',]
summary(Galton_data)['parent',,drop=FALSE]
summary(Galton_data)[1,,drop=FALSE]
summary(Galton_data)[,'father',drop=FALSE]
summary(Galton_data)[,'parent',drop=FALSE]
summary(Galton_data)[,'1,drop=FALSE]
summary(Galton_data)[,1,drop=FALSE]
summary(Galton_data)[,1,drop=FALSE]
p_h<- sum(Galton_data$parent)
summary(Galton_data)[1,,drop=FALSE]
rm(list=ls())
;
,
.
summary(Galton_data)[1,,drop=FALSE]
summary(Galton_data)[,1,drop=FALSE]
summary(Galton_data)[2,1,drop=FALSE]
summary(Galton_data)
p_h<- sum(Galton_data$parent)
c_h<-sum(Galton_data$child)
avg_height = ((p_h)+(c_h))/(nrow(Galton_data)*2)
x<-Galton_data$parent
y<-Galton_data$child
r = cor(x,y)
s_x = sd(x)
s_y= sd(y)
b_1<- r * (s_y/s_x)
b_0<- mean(y) - (b_1*(mean(x)))
simple_linear_model_formula = y ~ x
simple_linear_model = lm(simple_linear_model_formula)
Galton_data$gs<- b_0 + b_1 * x
Galton_data$residual <- y - Galton_data$gs
n<-nrow(Galton_data)
sse<- sum(Galton_data$residual**2)
mse<- sse/(n-2)
rmse<- sqrt(mse)
sse
mse
rmse
s_sq_y = var(Galton_data$child)
s_sq_e = var(Galton_data$residual)
rsq = (s_sq_y - s_sq_e) / s_sq_y
rsq
names(summary(simple_linear_model))
b = coef(simple_linear_model)
b
summary(simple_linear_model)$r.squared #the R^2
summary(simple_linear_model)$sigma #the RMSE
pacman::p_load(ggplot2)
simple_viz_obj = ggplot(Galton_data, aes(x, y)) +
geom_point(size = .8)
simple_ls_regression_line = geom_abline(intercept = b_0, slope = b_1, color = "red")
simple_viz_obj + simple_ls_regression_line
ggplot(Galton, aes(x = parent, y = child)) +
geom_point() +
geom_jitter() +
geom_abline(intercept = b_0, slope = b_1, color = "blue", size = 1) +
geom_abline(intercept = 0, slope = 1, color = "red", size = 1) +
geom_abline(intercept = avg_height, slope = 0, color = "darkgreen", size = 1) +
xlim(63.5, 72.5) +
ylim(63.5, 72.5) +
coord_equal(ratio = 1)
